#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\upshape\bfseries #1:}
\renewenvironment{description}{\list{}{%
  \setlength{\itemsep}{-2pt}
  \advance\leftmargini6\p@ \itemindent-12\p@
  \labelwidth\z@ \let\makelabel\descriptionlabel}%
}{
  \endlist
}

\usepackage{xcolor}
\usepackage{fancyhdr}

\definecolor{INSA_GM}{cmyk}{0.6,0,0,0}
\definecolor{INSA_GRIS}{cmyk}{0.7,0.6,0.5,0.3}
\definecolor{INSA_BLEU}{cmyk}{1,0.9,0.1,0}

\newcommand{\insertrefproj}[1]{}
\newcommand{\refproj}[1]{\renewcommand{\insertrefproj}{\textbf{\color{INSA_GRIS}#1}}}

\fancypagestyle{courant}{
\fancyhf{}
    \setlength{\headheight}{27pt}
    \fancyhead[L]{\raisebox{-2mm}{\includegraphics[width=30mm]{image/logoINSA.jpg}}}
\fancyhead[C]{}
\fancyhead[R]{\color{INSA_GRIS}\thepage}
\fancyfoot[L]{\insertrefproj}
\fancyfoot[R]{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.2pt}
}

\fancypagestyle{special}{
  \pagestyle{courant}
  \fancyfoot{}
\renewcommand{\footrulewidth}{0pt}
}

\fancypagestyle{plain}{
  \fancyhf{}
  \pagestyle{courant}
}
\title{Projet GM3}
\refproj{GM3/MIPP/2022 - 23}

\pagestyle{plain}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\begin_local_layout
Style Description
LabelIndent           MM
LeftMargin            MMMMMxx
End
\end_local_layout
\language french
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename ../../MIPP/lyx/IMAGE/logoINSA.jpg
	lyxscale 20
	width 100text%

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering 
\backslash
textcolor{INSA_GM}{
\end_layout

\end_inset


\series bold
\size larger
Projet MMSN 
\begin_inset Newline newline
\end_inset

GM3 2022 - 2023
\series default
\size default

\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset VSpace 7cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "c"
hor_pos "l"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "100text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\backslash
textcolor{INSA_BLEU}{
\end_layout

\end_inset


\series bold
\size giant
Gradient Conjugué 
\series default
\size default

\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset VSpace vfill
\end_inset


\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "l"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "6cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
large 
\backslash
textcolor{INSA_GRIS}{
\end_layout

\end_inset

 
\series bold
Étudiants :
\series default
 
\begin_inset Newline newline
\end_inset

LANGOLFF Clément
\begin_inset Newline newline
\end_inset

KESSLER Aymeric
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "l"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "9cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
large 
\backslash
textcolor{INSA_GRIS}{
\end_layout

\end_inset


\series bold
Enseignant-responsable du projet :
\series default

\begin_inset Newline newline
\end_inset

 
\noun on
El Bouchairi Imad
\noun default
 
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Introduction
\end_layout

\begin_layout Subsection
Fonctionnelle à minimiser
\end_layout

\begin_layout Standard
La méthode du gradient conjugué fait partie des méthodes de descente.
 À chaque itération, on determine un vecteur direction 
\begin_inset Formula $d_{k}$
\end_inset

 et un scalaire 
\begin_inset Formula $\alpha_{k}$
\end_inset

 permettant de calculer une nouvelle approche de la solution 
\begin_inset Formula $x_{k+1}=x_{k}+\alpha_{k}d_{k}$
\end_inset

.
 L'objectif des méthodes de descente est de minimiser la fonctionnelle
\begin_inset CommandInset citation
LatexCommand nocite
key "Lascaux2004-gj"
literal "false"

\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J(x)=\frac{1}{2}(Ax|x)-(b|x)=\frac{1}{2}x^{T}Ax-x^{T}b
\end{equation}

\end_inset

 où 
\begin_inset Formula $A$
\end_inset

 est une matrice symétrique définit positive ( 
\begin_inset Formula $A^{T}=A$
\end_inset

 et 
\begin_inset Formula $(Ax|x)>0\lor x\neq0$
\end_inset

).
 Dans ce cas, J est aussi définit positive et est quadratique.
 
\end_layout

\begin_layout Standard
Trouver le minimum de la fonctionnelle J revient à trouver la solution du
 système 
\begin_inset Formula $Ax=b$
\end_inset

.
 
\begin_inset Newline newline
\end_inset

En effet, comme J est quadratique est positive, J est connexe et son minimum
 unique 
\begin_inset Formula $\overline{x}$
\end_inset

 est obtenu en annulant le gradient de J : 
\begin_inset Formula $\nabla J(\overline{x})=A\overline{x}-b=0$
\end_inset

 
\begin_inset Formula $\iff\overline{x}$
\end_inset

 solution du système.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

On note 
\begin_inset Formula $r(x)=b-Ax=A(\overline{x}-x)$
\end_inset

 le résidu du système et 
\begin_inset Formula $e(x)=x-\overline{x}$
\end_inset

 l'erreur ou la différence entre la solution calculée et la solution exacte.
 Minimiser 
\begin_inset Formula $J$
\end_inset

 est équivalant à minimiser la fonctionnelle 
\begin_inset Formula $E$
\end_inset

 définit par
\begin_inset Formula 
\begin{equation}
E(x)=(A(x-\overline{x})|x-\overline{x})=(Ae(x)|e(x))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
En effet, 
\begin_inset Formula $E(x)=2J(x)+\underbrace{(A\overline{x}|\overline{x})}_{cst>0}$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Comme 
\begin_inset Formula $A$
\end_inset

 est symétrique définit positive, 
\begin_inset Formula $(Ax,y)=(x,Ay)$
\end_inset

 définit un produit scalaire et 
\begin_inset Formula $E(x)=(Ae(x)|e(x))=\parallel e(x)\parallel_{A}²$
\end_inset

 où 
\begin_inset Formula $\parallel e(x)\parallel_{A}$
\end_inset

 est la norme associée à ce produit scalaire.
 Dans la suite, nous utiliserons la fonctionnelle 
\begin_inset Formula $E$
\end_inset

 pour trouver la solution du système 
\begin_inset Formula $Ax=b$
\end_inset

.
\end_layout

\begin_layout Subsection
Choix optimal de 
\begin_inset Formula $\alpha_{k}$
\end_inset

 pour une direction fixée 
\begin_inset Formula $d_{k}$
\end_inset


\end_layout

\begin_layout Standard
On suppose que la direction 
\begin_inset Formula $d_{k}$
\end_inset

 est fixée.
 L'objectif est de minimiser la fonctionnelle 
\begin_inset Formula $E(x_{k})$
\end_inset

 à chaque itération.
 Or
\begin_inset Formula 
\begin{align*}
E(x_{k+1}) & =E(x_{k}+\alpha_{k}d_{k})=(A(x_{k}+\alpha_{k}d_{k}-\overline{x})|x_{k}+\alpha_{k}d_{k}-\overline{x})\\
 & =E(x_{k})-2\alpha_{k}(r_{k}|d_{k})+\alpha_{k}²(Ad_{k}|d_{k})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
qui est un polynôme de degrés 2 en 
\begin_inset Formula $\alpha_{k}$
\end_inset

 et atteint son minimum en 
\begin_inset Formula $\frac{-b}{2a}=\frac{(r_{k}|d_{k})}{(Ad_{k}|d_{k})}$
\end_inset

.
\end_layout

\begin_layout Standard
Ainsi, peut importe la direction 
\begin_inset Formula $d_{k}$
\end_inset

 choisie, le minimum de 
\begin_inset Formula $E$
\end_inset

 sera atteint pour 
\begin_inset Formula 
\begin{equation}
\alpha_{opt}=\frac{(r_{k}|d_{k})}{(Ad_{k}|d_{k})}
\end{equation}

\end_inset

.
\end_layout

\begin_layout Standard
De plus, on a 
\begin_inset Formula 
\begin{equation}
r_{k+1}=b-Ax_{k+1}=r_{k}-\alpha_{k}Ad_{k}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
et le résidu obtenu à l'itération 
\begin_inset Formula $k$
\end_inset

 est orthogonale à la direction 
\begin_inset Formula $d_{k}$
\end_inset

: 
\begin_inset Formula 
\begin{align}
(r_{k+1}|d_{k}) & =(r_{k}-\alpha_{k}Ad_{k}|d_{k})\nonumber \\
 & =(r_{k}|d_{k})-\alpha_{k}(Ad_{k}|d_{k})\nonumber \\
 & =(r_{k}|d_{k})-\frac{(r_{k}|d_{k})}{(Ad_{k}|d_{k})}(Ad_{k}|d_{k})\\
 & =0\nonumber 
\end{align}

\end_inset

.
 
\end_layout

\begin_layout Subsection
Choix de la direction optimal pour 
\begin_inset Formula $\alpha_{opt}$
\end_inset


\end_layout

\begin_layout Standard
Pour ce 
\begin_inset Formula $\alpha_{opt}$
\end_inset

, on a 
\begin_inset Formula 
\begin{align*}
E(x_{k+1}) & =E(x_{k})-2\alpha_{k}(r_{k}|d_{k})+\alpha_{k}²(Ad_{k}|d_{k})\\
 & =E(x_{k})-2\frac{(r_{k}|d_{k})}{(Ad_{k}|d_{k})}(r_{k}|d_{k})+\frac{(r_{k}|d_{k})²}{(Ad_{k}|d_{k})²}(Ad_{k}|d_{k})\\
 & =E(x_{k})-\frac{(r_{k}|d_{k})²}{(Ad_{k}|d_{k})}\\
 & =E(x_{k})(1-\frac{(r_{k}|d_{k})²}{E(x_{k})(Ad_{k}|d_{k})})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Or 
\begin_inset Formula $E(x_{k})=(Ae(x_{k})|e(x_{k}))$
\end_inset

 et 
\begin_inset Formula $r_{k}=A(x_{k}-\overline{x})=-Ae(x_{k})$
\end_inset


\begin_inset Formula $\iff e(x_{k})=-A^{-1}r_{k}$
\end_inset

.
 
\begin_inset Newline newline
\end_inset

Ainsi 
\begin_inset Formula $E(x_{k})=(A^{-1}r_{k}|r_{k})$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Donc 
\begin_inset Formula 
\begin{equation}
E(x_{k+1})=E(x_{k})(1-\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Mais 
\begin_inset Formula $E(x)$
\end_inset

 est une fonctionnelle minimisante qui s'approche le plus possible de la
 solution exacte 
\begin_inset Formula $\overline{x}$
\end_inset

, autrement dit on doit avoir 
\begin_inset Formula $E(x_{k+1})<E(x_{k})$
\end_inset

 et 
\begin_inset Formula $\underset{k\rightarrow+\infty}{lim}E(x_{k})=0$
\end_inset

.
\end_layout

\begin_layout Standard
On pose 
\begin_inset Formula $(u_{k})_{k\in\mathbb{N}}$
\end_inset

 la suite définit par 
\begin_inset Formula $u_{k}=$
\end_inset


\begin_inset Formula $1-\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}$
\end_inset

.
 On a 
\begin_inset Formula 
\begin{align*}
E(x_{k+1}) & <E(x_{k})u_{k}\\
E(x_{k+1}) & <u_{k}^{k+1}E(x_{0})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Donc 
\begin_inset Formula $E(x_{k})$
\end_inset

 définit une suite géométrique et converge si et seulement si à partir d'un
 certain rang 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $|u_{k}|=|1-\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}|<1$
\end_inset

.
 Autrement dit, s'il existe une constante 
\begin_inset Formula $\mu\in]0,1[$
\end_inset

 tel que 
\begin_inset Formula $\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}\geq\mu$
\end_inset

.
\end_layout

\begin_layout Standard
Or 
\begin_inset Formula $(Ad_{k}|d_{k})\leq\lambda_{1}||d_{k}||_{2}²$
\end_inset

 où 
\begin_inset Formula $\lambda_{1}$
\end_inset

 est la plus petite valeur propre de A.
 Et 
\begin_inset Formula $(A^{-1}r_{k}|r_{k})\leq\frac{1}{\lambda_{n}}||r_{k}||_{2}²$
\end_inset

 où 
\begin_inset Formula $\lambda_{n}$
\end_inset

 est la plus grande valeur propre de A.
 Donc 
\begin_inset Formula 
\[
\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}\geq\frac{\lambda_{1}}{\lambda_{n}}\frac{(r_{k}|d_{k})²}{||d_{k}||_{2}²||r_{k}||_{2}²}=\underbrace{\frac{1}{cond_{2}(A)}}_{\in]0,1[}\underbrace{(\frac{r_{k}}{||r_{k}||_{2}}|\frac{d_{k}}{||d_{k}||_{2}})²}_{\in[0,1]}
\]

\end_inset


\end_layout

\begin_layout Standard
Remarquons que 
\begin_inset Formula $E(x_{k})_{k}$
\end_inset

converge de plus en plus vite si 
\begin_inset Formula $u_{k}$
\end_inset

 est proche de 0, c'est a dire si 
\begin_inset Formula $\mu$
\end_inset

 est le plus proche possible de 1.
 Ainsi, en choisissant une direction colinéaire au résidu 
\begin_inset Formula $r_{k}$
\end_inset

, on a 
\begin_inset Formula 
\[
E(x_{k+1})<(\underbrace{1-\frac{1}{cond_{2}(A)}}_{\in]0,1[})^{k}E(x_{0})
\]

\end_inset


\end_layout

\begin_layout Standard
On a donc toujours une convergence de la suite 
\begin_inset Formula $E(x_{k})$
\end_inset

 et le conditionnement de la matrice A a une influence sur cette vitesse
 de convergence.
 Plus le conditionnement est petit, plus la convergence est rapide.
\end_layout

\begin_layout Standard
On a vu que 
\begin_inset Formula $\nabla J=-r$
\end_inset

 donc le gradient est colinéaire au résidu ( remarque 
\begin_inset Formula $\nabla E=-2r$
\end_inset

 aussi colinéaire au résidu ).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Motivation du gradient conjugué
\end_layout

\begin_layout Subsection
Analyse par méthode de projection
\end_layout

\begin_layout Standard
D'après le paragraphe d'avant, plus le conditionnement de la matrice est
 grand, plus la vitesse de convergence est faible.
 On souhaite donc maximiser 
\begin_inset Formula $\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}$
\end_inset

 peu importe le conditionnement de la matrice 
\begin_inset Formula $A$
\end_inset

.
 L'idée est de faire une combinaison linéaire du résidu et de la direction
 précédente pour trouver la direction qui se rapproche le plus du centre
 (la solution du système).
 la nouvelle direction se trouvera dans le plan généré par 
\begin_inset Formula $d_{k-1}$
\end_inset

et 
\begin_inset Formula $r_{k}$
\end_inset

.
 On pose 
\begin_inset Formula 
\begin{equation}
d_{k}=r_{k}+\beta_{k}d_{k-1}
\end{equation}

\end_inset

.
\end_layout

\begin_layout Standard
Comme 
\begin_inset Formula $(r_{k+1}|d_{k})=0$
\end_inset

 , on a 
\begin_inset Formula 
\begin{align*}
\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})} & =\frac{(r_{k}|r_{k}+\beta_{k}d_{k-1})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}\\
 & =\frac{((r_{k}|r_{k})+\beta_{k}(r_{k}|d_{k-1}))²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}\\
 & =\frac{(r_{k}|r_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Ainsi maximiser 
\begin_inset Formula $\frac{(r_{k}|d_{k})²}{(A^{-1}r_{k}|r_{k})(Ad_{k}|d_{k})}$
\end_inset

 revient à minimiser 
\begin_inset Formula $(Ad_{k}|d_{k})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
(Ad_{k}|d_{k}) & =(A(r_{k}+\beta_{k}d_{k-1})|r_{k}+\beta_{k}d_{k-1})\\
 & =\beta_{k}²(Ad_{k-1}|d_{k-1})+2\beta_{k}(Ad_{k-1}|r_{k})+(Ar_{k}|r_{k})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
trinôme en 
\begin_inset Formula $\beta_{k}$
\end_inset

 dont le coefficient sur le degrés principale est positif, le minimum est
 atteint en 
\begin_inset Formula $\frac{-b}{2a}=-\frac{(Ad_{k-1}|r_{k})}{(Ad_{k-1}|d_{k-1})}$
\end_inset

.
\end_layout

\begin_layout Standard
Ainsi la direction maximisant la vitesse de convergence est obtenue pour
 
\begin_inset Formula 
\begin{equation}
\beta_{k}=-\frac{(Ad_{k-1}|r_{k})}{(Ad_{k-1}|d_{k-1})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
De plus, on a 
\begin_inset Formula 
\begin{align}
(Ad_{k-1}|d_{k}) & =(Ad_{k-1}|r_{k}+\beta_{k}d_{k-1})\nonumber \\
 & =(Ad_{k-1}|r_{k})-\frac{(Ad_{k-1}|r_{k})}{(Ad_{k-1}|d_{k-1})}(Ad_{k-1}|d_{k-1})\\
 & =0\nonumber 
\end{align}

\end_inset

 On dit que les directions choisies à chaque itération sont A-conjugués.
 
\end_layout

\begin_layout Standard
Et 
\begin_inset Formula 
\begin{align}
(r_{k}|d_{k}) & =(r_{k}|r_{k}+\beta_{k}d_{k-1})\nonumber \\
 & =(r_{k}|r_{k})+\beta_{k}\underbrace{(r_{k}|d_{k-1})}_{0}\\
 & =(r_{k}|r_{k})\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
On peut récrire 
\begin_inset Formula $\beta_{k}$
\end_inset

 en fonction du résidu :
\end_layout

\begin_layout Standard
on sait que 
\begin_inset Formula $r_{k+1}=r_{k}-\alpha_{k}Ad_{k}$
\end_inset

 donc 
\begin_inset Formula $Ad_{k-1}=\frac{1}{\alpha_{k-1}}(r_{k-1}-r_{k})$
\end_inset


\end_layout

\begin_layout Standard
et comme 
\begin_inset Formula 
\begin{align}
(r_{k+1}|r_{k}) & =(r_{k}-\alpha_{k}Ad_{k}|r_{k})\nonumber \\
 & =(r_{k}|r_{k})-\alpha_{k}(Ad_{k},r_{k})\nonumber \\
 & =(r_{k}|r_{k})-\alpha_{k}(Ad_{k},d_{k}-\beta_{k}d_{k-1})\nonumber \\
 & =(r_{k}|r_{k})-\alpha_{k}(Ad_{k},d_{k})+\alpha_{k}\beta_{k}\underbrace{(Ad_{k}|d_{k-1})}_{0}\\
 & =(r_{k}|r_{k})-\frac{(r_{k}|d_{k})}{(Ad_{k}|d_{k})}(Ad_{k},d_{k})\nonumber \\
 & =0\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Donc
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\beta_{k} & =-\frac{(\frac{1}{\alpha_{k-1}}(r_{k-1}-r_{k})|r_{k})}{(\frac{1}{\alpha_{k-1}}(r_{k-1}-r_{k})|d_{k-1})}\nonumber \\
 & =-\frac{\overbrace{(r_{k-1}|r_{k})}^{0}-(r_{k}|r_{k})}{(r_{k-1}|d_{k-1})-\underbrace{(r_{k}|d_{k-1})}_{0}}\\
 & =\frac{(r_{k}|r_{k})}{(r_{k-1}|d_{k-1})}\nonumber \\
 & =\frac{(r_{k}|r_{k})}{(r_{k-1}|r_{k-1})}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Algorithme pour la méthode du gradient conjugué
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Fonction GradConj (Nmax,tol,A,x,r0)
\end_layout

\begin_layout Plain Layout

	d = r0
\end_layout

\begin_layout Plain Layout

	normeR0 = (r0,r0)
\end_layout

\begin_layout Plain Layout

	TANT QUE normeR0 < tol FAIRE
\end_layout

\begin_layout Plain Layout

		Ad = A*d
\end_layout

\begin_layout Plain Layout

		alpha = normeR0 / (Ad,d)
\end_layout

\begin_layout Plain Layout

		x = x + alpha * d
\end_layout

\begin_layout Plain Layout

		r1 = r0 - alpha * Ad
\end_layout

\begin_layout Plain Layout

		normeR1 = (r1,r1)
\end_layout

\begin_layout Plain Layout

		beta = normeR1 / normeR0
\end_layout

\begin_layout Plain Layout

		d = r1 + beta * d 
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Applications
\end_layout

\begin_layout Subsection
Étude en dimension 2
\end_layout

\begin_layout Standard
En dimension 2, la fonction bilinéaire symétrique définit par la matrice
 A est une paraboloïde et les lignes de niveau sont des ellipses.
 En dimension 2, l'algorithme du gradient conjugué doit converger en seulement
 2 itérations peu importe le conditionnement de la matrice.
 A l'inverse, si le conditionnement de la matrice est mauvais, l'algorithme
 du gradient descente a une vitesse de convergence très mauvaise.
 On peut interpréter cette mauvaise convergence en observant des 
\begin_inset Quotes fld
\end_inset

sauts de puce
\begin_inset Quotes frd
\end_inset

 entre les courbes de niveau de la fonction.
 En effet, si la matrice est mal conditionnée, ces ellipses sont très allongées
 ce qui implique un déplacement très petit puisque la direction de descente
 est choisie orthogonale à la ligne de niveau.
\end_layout

\begin_layout Standard
Considérons la matrice A =
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{math}
\backslash
displaystyle 
\backslash
left(
\backslash
begin{array}{rr} 
\backslash
 2 & -1 
\backslash

\backslash
 -1 & 2 
\backslash
end{array}
\backslash
right)
\backslash
end{math}
\end_layout

\end_inset

 et b = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{math}
\backslash
displaystyle 
\backslash
left(
\backslash
begin{array}{rr} 
\backslash
 1 
\backslash

\backslash
 1 
\backslash
end{array}
\backslash
right)
\backslash
end{math}
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Conditionnement de A = 2.99999
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename image/gradientconj.eps
	lyxscale 50
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
Algorithme du gradient conjugué appliqué à la matrice A pour 
\begin_inset Formula $x_{0}=(-5,-10)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename image/descente.eps
	lyxscale 50
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
Algorithme du gradient descente appliqué à la matrice A pour 
\begin_inset Formula $x_{0}=(-5,-10)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Application en dimension N
\end_layout

\begin_layout Standard
On se place maintenant dans le cas infini A = 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{math}
\backslash
displaystyle 
\backslash
begin{pmatrix} 2 & -1 & 0 & 
\backslash
cdots & 0 
\backslash

\backslash
 -1 & 2 & -1 & 
\backslash
ddots & 
\backslash
vdots 
\backslash

\backslash
 0 & -1 & 
\backslash
ddots & 
\backslash
ddots & 0 
\backslash

\backslash
 
\backslash
vdots & 
\backslash
ddots & 
\backslash
ddots & 2 & -1 
\backslash

\backslash
 0 & 
\backslash
cdots & 0 & -1 & 2
\backslash
end{pmatrix}
\backslash
end{math}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Conditionnement de A = 16373
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
center
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename image/comparaisonGrad.eps
	lyxscale 50
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout
Comparaison des vitesses de convergence entre l'algorithme du gradient descente
 (en bleu) et du gradient conjugué (en rouge) pour N = 200
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
On remarque que l'algorithme du gradient conjugué converge beaucoup plus
 vite que l'algorithme de descente.
 De plus, on obtient une solution précise bien avant la dimension de la
 matrice.
 En moins de 100 itérations, on atteint déjà un résidu inférieur à 
\begin_inset Formula $10^{-10}$
\end_inset

 alors que le résidu du gradient descente stagne encore après 200 itérations.
 
\begin_inset CommandInset citation
LatexCommand nocite
key "Amodei2008-gp"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand nocite
key "Meurisse2018-fj"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand nocite
key "Magoules2017-tb"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Part
Conclusion
\end_layout

\begin_layout Standard
L'algorithme du gradient conjugué fait partie des techniques les plus efficaces
 pour trouver la solution au système 
\begin_inset Formula $Ax=b$
\end_inset

.
 On peut encore améliorer cette convergence rapide avec un bon conditionnement
 de la matrice.
 Bien que cette méthode soit itérative, elle peut être considérée comme
 méthode directe puisqu'elle fournit la solution exacte en au plus n itérations
 ( voir moins ).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
